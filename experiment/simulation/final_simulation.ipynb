{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTQyo1IREMyC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import pi, floor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "from torch.autograd import Variable\n",
    "import core\n",
    "from fft import roll, fftshift, ifftshift\n",
    "from torch.fft import ifft, fft, ifftn, fftn\n",
    "from IQA_pytorch import SSIM, VIF\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "attack_path = 'result/simulation/' # + \"name_of_model/image\"\n",
    "slm_path = 'result/simulation/' # + \"name_of_model/slm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtUvm2smO2ge"
   },
   "outputs": [],
   "source": [
    "GPU_NUM = 0 # GPU Num\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1_IYpAMO2gf"
   },
   "outputs": [],
   "source": [
    "## Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XNn41YWO2gg"
   },
   "outputs": [],
   "source": [
    "# Choose the pytorch model for which you want to generate adversarial attack\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50.eval()\n",
    "resnet50.to(device)\n",
    "\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.eval()\n",
    "# vgg16.to(device)\n",
    "\n",
    "# mobilenet = models.mobilenet_v3_large(pretrained=True)\n",
    "# mobilenet.eval()\n",
    "# mobilenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4X_uwKbO2gg"
   },
   "outputs": [],
   "source": [
    "## store data\n",
    "data=np.zeros((1000,3,224,224)).astype('float32') # Reshape the data as per the input size of Models\n",
    "labels=(np.zeros(1000))\n",
    "labels=labels.astype(int)\n",
    "\n",
    "\n",
    "## mean and std will remain same irresptive of the model you use\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess11 = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "def normalize(t):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - mean[0])/std[0]\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - mean[1])/std[1]\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - mean[2])/std[2]\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "## readable labels (download from https://savan77.github.io/blog/imagenet_adv_examples.html)\n",
    "labels_link = \"https://savan77.github.io/blog/labels.json\"\n",
    "labels_json = requests.get(labels_link).json()\n",
    "labells = {int(idx): label for idx, label in labels_json.items()}\n",
    "\n",
    "\n",
    "## loading data\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "def torch_to_np(img_var):\n",
    "    return img_var.detach().cpu().numpy()[0]\n",
    "\n",
    "def load(path):\n",
    "    img = Image.open(path)\n",
    "    return img\n",
    "\n",
    "\n",
    "## IQA\n",
    "model1 = SSIM(channels=3)\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = torch.mean((img1 * 255 - img2 * 255) ** 2)\n",
    "    return 20 * torch.log10(255.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padsize = 1\n",
    "m = 224  #Pixel number of SLM\n",
    "factor = 2  #For Nyquist sampling\n",
    "factor2 = 1\n",
    "M = m * factor * factor2  #Image size\n",
    "f = 100000  #Focal length of relay lens in um\n",
    "length = [0.61, 0.53, 0.47]  #Wavelength in um [Red Green Blue]. Note that center wavelegth is green color.\n",
    "length = np.array(length)\n",
    "fov = 250000  #Field-of-view in um\n",
    "eff_pixelsize = fov/M  #Effective pixel size\n",
    "slm_pixelsize = 30  #Pixel size of SLM in um\n",
    "slm_pixelnum = m  #추가\n",
    "\n",
    "PhySizeSLM = slm_pixelnum * slm_pixelsize\n",
    "MaxFreqSLM = PhySizeSLM / length / f / 2\n",
    "DelFreqSLM = slm_pixelsize / np.min(length) / f  #기준은 가장 High frequency를 전달할 수 있는 Blue channel\n",
    "\n",
    "kx = (DelFreqSLM * torch.range(-M / 2, M / 2 - 1)).to(torch.double)\n",
    "ky = (DelFreqSLM * torch.range(-M / 2, M / 2 - 1)).to(torch.double)\n",
    "kxm, kym = torch.meshgrid(kx, ky)\n",
    "\n",
    "Ratio = MaxFreqSLM / MaxFreqSLM[2]  #Radius ratio of each color channel\n",
    "\n",
    "cent = M/2\n",
    "initial = torch.rand(224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random perturbation\n",
    "\n",
    "class FourierDomain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierDomain, self).__init__()\n",
    "        self.MM = torch.nn.Parameter(initial)\n",
    "        self.MM.requires_grad = True\n",
    "\n",
    "    def forward(self, inputIntensityFT, attack):\n",
    "        slm = self.MM\n",
    "        \n",
    "        slm = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=factor2)))\n",
    "        CTF_B = (torch.sqrt(torch.pow(kxm, 2) + torch.pow(kym, 2)) < MaxFreqSLM[2] * factor2).to(torch.double)\n",
    "        CTF_B = CTF_B.to(device)\n",
    "        slm = slm * CTF_B[int(cent-M/4):int(cent+M/4),int(cent-M/4):int(cent+M/4)]  #Physicially displayed SLM pattern\n",
    "        \n",
    "        slm_R = Ratio[0] * torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=Ratio[0])))\n",
    "        slm_G = Ratio[1] * torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=Ratio[1])))\n",
    "        slm_B = slm\n",
    "        \n",
    "        slm_r = torch.zeros(M, M).to(device)\n",
    "        slm_g = torch.zeros(M, M).to(device)\n",
    "        slm_b = torch.zeros(M, M).to(device)\n",
    "        \n",
    "        x, y = tuple(list(slm_R.size()))\n",
    "        slm_r[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_R\n",
    "\n",
    "        x, y = tuple(list(slm_G.size()))\n",
    "        slm_g[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_G\n",
    "\n",
    "        x, y = tuple(list(slm_B.size()))\n",
    "        slm_b[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_B\n",
    "        \n",
    "        CTF_R = (slm_r != 0).to(torch.double)\n",
    "        CTF_G = (slm_g != 0).to(torch.double)\n",
    "        CTF_B = (slm_b != 0).to(torch.double)\n",
    "        \n",
    "        atk_CTF_R_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_r), 2), torch.unsqueeze(torch.sin(slm_r), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_R_mask = torch.view_as_complex(atk_CTF_R_mask_stack)\n",
    "        atk_CTF_R = CTF_R * atk_CTF_R_mask\n",
    "\n",
    "        atk_CTF_G_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_g), 2), torch.unsqueeze(torch.sin(slm_g), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_G_mask = torch.view_as_complex(atk_CTF_G_mask_stack)\n",
    "        atk_CTF_G = CTF_G * atk_CTF_G_mask\n",
    "\n",
    "        atk_CTF_B_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_b), 2), torch.unsqueeze(torch.sin(slm_b), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_B_mask = torch.view_as_complex(atk_CTF_B_mask_stack)\n",
    "        atk_CTF_B = CTF_B * atk_CTF_B_mask\n",
    "        \n",
    "        if attack == True:\n",
    "            ft_atk_CTF_R = fftn(fftshift(atk_CTF_R))\n",
    "            MTF_R = ifftshift(ifftn((ft_atk_CTF_R * torch.conj(ft_atk_CTF_R))))\n",
    "\n",
    "            ft_atk_CTF_G = fftn(fftshift(atk_CTF_G))\n",
    "            MTF_G = ifftshift(ifftn((ft_atk_CTF_G * torch.conj(ft_atk_CTF_G))))\n",
    "\n",
    "            ft_atk_CTF_B = fftn(fftshift(atk_CTF_B))\n",
    "            MTF_B = ifftshift(ifftn((ft_atk_CTF_B * torch.conj(ft_atk_CTF_B))))\n",
    "\n",
    "        elif attack == False:\n",
    "            ft_CTF_R = fftn(fftshift(CTF_R))\n",
    "            MTF_R = ifftshift(ifftn((ft_CTF_R * torch.conj(ft_CTF_R))))\n",
    "\n",
    "            ft_CTF_G = fftn(fftshift(CTF_G))\n",
    "            MTF_G = ifftshift(ifftn((ft_CTF_G * torch.conj(ft_CTF_G))))\n",
    "\n",
    "            ft_CTF_B = fftn(fftshift(CTF_B))\n",
    "            MTF_B = ifftshift(ifftn((ft_CTF_B * torch.conj(ft_CTF_B))))\n",
    "\n",
    "        MTF_R = MTF_R / torch.max(torch.max(torch.abs(MTF_R)))\n",
    "        MTF_G = MTF_G / torch.max(torch.max(torch.abs(MTF_G)))\n",
    "        MTF_B = MTF_B / torch.max(torch.max(torch.abs(MTF_B)))\n",
    "\n",
    "        #inputIntensityFT = torch.as_tensor(inputIntensityFT, dtype=torch.cdouble)\n",
    "        inputIntensityFT_R = inputIntensityFT[:, :, 0]\n",
    "        inputIntensityFT_G = inputIntensityFT[:, :, 1]\n",
    "        inputIntensityFT_B = inputIntensityFT[:, :, 2]\n",
    "        \n",
    "        outputFT_R = MTF_R * inputIntensityFT_R\n",
    "        outputFT_G = MTF_G * inputIntensityFT_G\n",
    "        outputFT_B = MTF_B * inputIntensityFT_B\n",
    "        \n",
    "        ifft_R = torch.abs(ifftn(ifftshift(outputFT_R))).to(torch.float32) / 255\n",
    "        ifft_G = torch.abs(ifftn(ifftshift(outputFT_G))).to(torch.float32) / 255\n",
    "        ifft_B = torch.abs(ifftn(ifftshift(outputFT_B))).to(torch.float32) / 255\n",
    "                \n",
    "        output_R = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_R, 0), 0), size=(224, 224))))\n",
    "        output_G = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_G, 0), 0), size=(224, 224))))\n",
    "        output_B = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_B, 0), 0), size=(224, 224))))\n",
    "    \n",
    "        image_tensor2 = torch.stack([output_R, output_G, output_B])\n",
    "        \n",
    "    \n",
    "        return image_tensor2, slm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv(\"images.csv\") \n",
    "path= 'clean_images' # download from https://kaggle.com/c/6864\n",
    "\n",
    "for i in range(1000):\n",
    "    ImgID= images.iloc[i,0] \n",
    "    img = load(path +'/'+ ImgID + '.png')\n",
    "    img = img.resize((224,224), Image.ANTIALIAS) # Change the input image shape here\n",
    "    img = to_tensor(img)[None, :]\n",
    "    \n",
    "    img_np = torch_to_np(img)\n",
    "    data[i,:,:,:] = img_np\n",
    "    labels[i]= images.iloc[i,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcztXDdiEMyS"
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    ImgID= images.iloc[i,0] \n",
    "    img= data[i]\n",
    "    img= torch.from_numpy(img)[None,:]\n",
    "    label= labels[i]-1 # the first label is redundant\n",
    "    label= torch.tensor([label] , dtype=torch.int64)\n",
    "    img, label = img.to(device), label.to(device)\n",
    "    \n",
    "    IMG2 = core.imresize(img, sizes=(M, M))\n",
    "    inputIntensityFT_R = fftshift(fftn(IMG2[:,0,:,:])).squeeze(0)\n",
    "    inputIntensityFT_G = fftshift(fftn(IMG2[:,1,:,:])).squeeze(0)\n",
    "    inputIntensityFT_B = fftshift(fftn(IMG2[:,2,:,:])).squeeze(0)\n",
    "    inputIntensityFT = torch.stack([inputIntensityFT_R, inputIntensityFT_G, inputIntensityFT_B])\n",
    "    inputIntensityFT = (inputIntensityFT * 255).permute(1, 2, 0)\n",
    "    inputIntensityFT = inputIntensityFT.detach().to(device)\n",
    "    \n",
    "    initial = torch.rand(224, 224).to(device)\n",
    "    model = FourierDomain()\n",
    "    model.to(device)\n",
    "    \n",
    "    image_tensor, slm = model(inputIntensityFT, attack=False)\n",
    "    image_tensor = preprocess11(image_tensor)  # preprocess an i\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # add batch dimension.  C X H X W ==> B X C X H X W\n",
    "    image_tensor = image_tensor.detach()\n",
    "    del initial, model, slm\n",
    "    \n",
    "    x1 = image_tensor.squeeze(0)\n",
    "    x1 = x1.mul((torch.FloatTensor(std)).to(device).view(3, 1, 1)).add(\n",
    "        (torch.FloatTensor(mean)).to(device).view(3, 1, 1)).cpu().detach().numpy()  # reverse of normalization op- \"unnormalize\"\n",
    "    x1 = np.transpose(x1, (1, 2, 0))  # C X H X W  ==>   H X W X C\n",
    "    x1 = np.clip(x1, 0, 1)\n",
    "    ref = torch.as_tensor(x1).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    target = Variable(torch.LongTensor([label.item()]).to(device), requires_grad=False)\n",
    "    \n",
    "    x_adv_pred = labells[label.item()]\n",
    "    param = 1e-2\n",
    "    n_epoch = 300\n",
    "    a = 0\n",
    "    \n",
    "    while x_adv_pred == labells[label.item()]:\n",
    "        \n",
    "        initial = torch.rand(224, 224).to(device)\n",
    "        model = FourierDomain()\n",
    "        model.to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5 * (1e-3), weight_decay=5e-6)\n",
    "        \n",
    "        for epoch in range(0, n_epoch):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            image_tensor2, slm = model(inputIntensityFT, attack=True)\n",
    "            image_tensor2 = preprocess11(image_tensor2)\n",
    "            image_tensor2 = image_tensor2.unsqueeze(0)\n",
    "            output2 = mobilenet.forward(image_tensor2)\n",
    "            \n",
    "            # perform a backward pass in order to get gradients\n",
    "            loss2 = param * torch.norm(image_tensor2 - image_tensor) - criterion(output2, target)\n",
    "            \n",
    "            x2 = image_tensor2.squeeze(0)\n",
    "            x2 = x2.mul((torch.FloatTensor(std)).to(device).view(3, 1, 1)).add(\n",
    "                (torch.FloatTensor(mean)).to(device).view(3, 1, 1)).cpu().detach().numpy()  # reverse of normalization op- \"unnormalize\"\n",
    "            x2 = np.transpose(x2, (1, 2, 0))  # C X H X W  ==>   H X W X C\n",
    "            x2 = np.clip(x2, 0, 1)\n",
    "            dist = torch.as_tensor(x2).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            pp = psnr(dist, ref).item()\n",
    "            ss = model1(dist, ref, as_loss=False).item()\n",
    "            \n",
    "            im = Image.fromarray(np.uint8(x2 * 255)).convert('RGB')\n",
    "            imm = to_tensor(im)[None, :]\n",
    "            imm = imm.to(device)\n",
    "            output2 = mobilenet.forward(preprocess11(imm))\n",
    "            x_adv_pred = labells[torch.max(output2.data, 1)[1][0].item()]\n",
    "            \n",
    "            if x_adv_pred != labells[label.item()]:\n",
    "                break\n",
    "\n",
    "            loss2.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        if x_adv_pred != labells[label.item()]:\n",
    "            break\n",
    "        \n",
    "        del initial, model, criterion, optimizer, loss2\n",
    "        \n",
    "        param /= 10\n",
    "        a += 1\n",
    "        \n",
    "        if a == 6:\n",
    "            break\n",
    "    \n",
    "    print(i, ' ', param, ' ', pp, ' ', ss)\n",
    "    im.save(attack_path+ImgID+'.png')\n",
    "    MM = slm.cpu().detach().numpy()\n",
    "    df = pd.DataFrame(MM)\n",
    "    df.to_csv(slm_path+ImgID+'.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_simulation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
