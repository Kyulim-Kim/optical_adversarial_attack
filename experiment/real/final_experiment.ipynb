{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4216,
     "status": "ok",
     "timestamp": 1620638194576,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "iTQyo1IREMyC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import random\n",
    "from math import pi, floor\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import requests, io\n",
    "from torch.autograd import Variable\n",
    "import scipy.misc\n",
    "from torch.fft import ifft, fft, ifftn, fftn\n",
    "from IQA_pytorch import SSIM, VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'name_of_image.png'\n",
    "name2 = 'name_of_image.xlsx'\n",
    "\n",
    "filepath = 'image/' + name\n",
    "adv_filepath = 'result/real/' # + \"name_of_model/\" + name\n",
    "adv_filepath2 = 'result/real/' # + \"name_of_model/\" + name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17732,
     "status": "ok",
     "timestamp": 1620638208102,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "lyWv4ZtkKFo1"
   },
   "outputs": [],
   "source": [
    "def fft2(data):\n",
    "    assert data.size(-1) == 2\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fft2c(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2 dimensional Fast Fourier Transform.\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "    Returns:\n",
    "        torch.Tensor: The FFT of the input.\n",
    "    \"\"\"\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2(data):\n",
    "    assert data.size(-1) == 2\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2c(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2-dimensional Inverse Fast Fourier Transform.\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "    Returns:\n",
    "        torch.Tensor: The IFFT of the input.\n",
    "    \"\"\"\n",
    "    assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def roll(x, shift, dim):\n",
    "    \"\"\"\n",
    "    Similar to np.roll but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if isinstance(shift, (tuple, list)):\n",
    "        assert len(shift) == len(dim)\n",
    "        for s, d in zip(shift, dim):\n",
    "            x = roll(x, s, d)\n",
    "        return x\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "\n",
    "def fftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.fftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [dim // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = x.shape[dim] // 2\n",
    "    else:\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.ifftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = (x.shape[dim] + 1) // 2\n",
    "    else:\n",
    "        shift = [(x.shape[i] + 1) // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19066,
     "status": "ok",
     "timestamp": 1620638209440,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "98Xsl0RGKIk-",
    "outputId": "0632b3bb-9bf4-4d09-e9c3-f9839da46860"
   },
   "outputs": [],
   "source": [
    "__all__ = ['imresize'] \n",
    "\n",
    "_I = typing.Optional[int]\n",
    "_D = typing.Optional[torch.dtype]\n",
    "\n",
    "def nearest_contribution(x: torch.Tensor) -> torch.Tensor:\n",
    "    range_around_0 = torch.logical_and(x.gt(-0.5), x.le(0.5))\n",
    "    cont = range_around_0.to(dtype=x.dtype)\n",
    "    return cont\n",
    "\n",
    "def linear_contribution(x: torch.Tensor) -> torch.Tensor:\n",
    "    ax = x.abs()\n",
    "    range_01 = ax.le(1)\n",
    "    cont = (1 - ax) * range_01.to(dtype=x.dtype)\n",
    "    return cont\n",
    "\n",
    "def cubic_contribution(x: torch.Tensor, a: float=-0.5) -> torch.Tensor:\n",
    "    ax = x.abs()\n",
    "    ax2 = ax * ax\n",
    "    ax3 = ax * ax2\n",
    "\n",
    "    range_01 = ax.le(1)\n",
    "    range_12 = torch.logical_and(ax.gt(1), ax.le(2))\n",
    "\n",
    "    cont_01 = (a + 2) * ax3 - (a + 3) * ax2 + 1\n",
    "    cont_01 = cont_01 * range_01.to(dtype=x.dtype)\n",
    "\n",
    "    cont_12 = (a * ax3) - (5 * a * ax2) + (8 * a * ax) - (4 * a)\n",
    "    cont_12 = cont_12 * range_12.to(dtype=x.dtype)\n",
    "\n",
    "    cont = cont_01 + cont_12\n",
    "    return cont\n",
    "\n",
    "def gaussian_contribution(x: torch.Tensor, sigma: float=2.0) -> torch.Tensor:\n",
    "    range_3sigma = (x.abs() <= 3 * sigma + 1)\n",
    "    # Normalization will be done after\n",
    "    cont = torch.exp(-x.pow(2) / (2 * sigma**2))\n",
    "    cont = cont * range_3sigma.to(dtype=x.dtype)\n",
    "    return cont\n",
    "\n",
    "def discrete_kernel(\n",
    "        kernel: str, scale: float, antialiasing: bool=True) -> torch.Tensor:\n",
    "\n",
    "    '''\n",
    "    For downsampling with integer scale only.\n",
    "    '''\n",
    "    downsampling_factor = int(1 / scale)\n",
    "    if kernel == 'cubic':\n",
    "        kernel_size_orig = 4\n",
    "    else:\n",
    "        raise ValueError('Pass!')\n",
    "\n",
    "    if antialiasing:\n",
    "        kernel_size = kernel_size_orig * downsampling_factor\n",
    "    else:\n",
    "        kernel_size = kernel_size_orig\n",
    "\n",
    "    if downsampling_factor % 2 == 0:\n",
    "        a = kernel_size_orig * (0.5 - 1 / (2 * kernel_size))\n",
    "    else:\n",
    "        kernel_size -= 1\n",
    "        a = kernel_size_orig * (0.5 - 1 / (kernel_size + 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        r = torch.linspace(-a, a, steps=kernel_size)\n",
    "        k = cubic_contribution(r).view(-1, 1)\n",
    "        k = torch.matmul(k, k.t())\n",
    "        k /= k.sum()\n",
    "\n",
    "    return k\n",
    "\n",
    "def reflect_padding(\n",
    "        x: torch.Tensor,\n",
    "        dim: int,\n",
    "        pad_pre: int,\n",
    "        pad_post: int) -> torch.Tensor:\n",
    "\n",
    "    b, c, h, w = x.size()\n",
    "    if dim == 2 or dim == -2:\n",
    "        padding_buffer = x.new_zeros(b, c, h + pad_pre + pad_post, w)\n",
    "        padding_buffer[..., pad_pre:(h + pad_pre), :].copy_(x)\n",
    "        for p in range(pad_pre):\n",
    "            padding_buffer[..., pad_pre - p - 1, :].copy_(x[..., p, :])\n",
    "        for p in range(pad_post):\n",
    "            padding_buffer[..., h + pad_pre + p, :].copy_(x[..., -(p + 1), :])\n",
    "    else:\n",
    "        padding_buffer = x.new_zeros(b, c, h, w + pad_pre + pad_post)\n",
    "        padding_buffer[..., pad_pre:(w + pad_pre)].copy_(x)\n",
    "        for p in range(pad_pre):\n",
    "            padding_buffer[..., pad_pre - p - 1].copy_(x[..., p])\n",
    "        for p in range(pad_post):\n",
    "            padding_buffer[..., w + pad_pre + p].copy_(x[..., -(p + 1)])\n",
    "\n",
    "    return padding_buffer\n",
    "\n",
    "def padding(\n",
    "        x: torch.Tensor,\n",
    "        dim: int,\n",
    "        pad_pre: int,\n",
    "        pad_post: int,\n",
    "        padding_type: typing.Optional[str]='reflect') -> torch.Tensor:\n",
    "\n",
    "    if padding_type is None:\n",
    "        return x\n",
    "    elif padding_type == 'reflect':\n",
    "        x_pad = reflect_padding(x, dim, pad_pre, pad_post)\n",
    "    else:\n",
    "        raise ValueError('{} padding is not supported!'.format(padding_type))\n",
    "\n",
    "    return x_pad\n",
    "\n",
    "def get_padding(\n",
    "        base: torch.Tensor,\n",
    "        kernel_size: int,\n",
    "        x_size: int) -> typing.Tuple[int, int, torch.Tensor]:\n",
    "\n",
    "    base = base.long()\n",
    "    r_min = base.min()\n",
    "    r_max = base.max() + kernel_size - 1\n",
    "\n",
    "    if r_min <= 0:\n",
    "        pad_pre = -r_min\n",
    "        pad_pre = pad_pre.item()\n",
    "        base += pad_pre\n",
    "    else:\n",
    "        pad_pre = 0\n",
    "\n",
    "    if r_max >= x_size:\n",
    "        pad_post = r_max - x_size + 1\n",
    "        pad_post = pad_post.item()\n",
    "    else:\n",
    "        pad_post = 0\n",
    "\n",
    "    return pad_pre, pad_post, base\n",
    "\n",
    "def get_weight(\n",
    "        dist: torch.Tensor,\n",
    "        kernel_size: int,\n",
    "        kernel: str='cubic',\n",
    "        sigma: float=2.0,\n",
    "        antialiasing_factor: float=1) -> torch.Tensor:\n",
    "\n",
    "    buffer_pos = dist.new_zeros(kernel_size, len(dist))\n",
    "    for idx, buffer_sub in enumerate(buffer_pos):\n",
    "        buffer_sub.copy_(dist - idx)\n",
    "\n",
    "    # Expand (downsampling) / Shrink (upsampling) the receptive field.\n",
    "    buffer_pos *= antialiasing_factor\n",
    "    if kernel == 'cubic':\n",
    "        weight = cubic_contribution(buffer_pos)\n",
    "    elif kernel == 'gaussian':\n",
    "        weight = gaussian_contribution(buffer_pos, sigma=sigma)\n",
    "    else:\n",
    "        raise ValueError('{} kernel is not supported!'.format(kernel))\n",
    "\n",
    "    weight /= weight.sum(dim=0, keepdim=True)\n",
    "    return weight\n",
    "\n",
    "def reshape_tensor(x: torch.Tensor, dim: int, kernel_size: int) -> torch.Tensor:\n",
    "    # Resize height\n",
    "    if dim == 2 or dim == -2:\n",
    "        k = (kernel_size, 1)\n",
    "        h_out = x.size(-2) - kernel_size + 1\n",
    "        w_out = x.size(-1)\n",
    "    # Resize width\n",
    "    else:\n",
    "        k = (1, kernel_size)\n",
    "        h_out = x.size(-2)\n",
    "        w_out = x.size(-1) - kernel_size + 1\n",
    "\n",
    "    unfold = F.unfold(x, k)\n",
    "    unfold = unfold.view(unfold.size(0), -1, h_out, w_out)\n",
    "    return unfold\n",
    "\n",
    "def reshape_input(\n",
    "        x: torch.Tensor) -> typing.Tuple[torch.Tensor, _I, _I, _I, _I]:\n",
    "\n",
    "    if x.dim() == 4:\n",
    "        b, c, h, w = x.size()\n",
    "    elif x.dim() == 3:\n",
    "        c, h, w = x.size()\n",
    "        b = None\n",
    "    elif x.dim() == 2:\n",
    "        h, w = x.size()\n",
    "        b = c = None\n",
    "    else:\n",
    "        raise ValueError('{}-dim Tensor is not supported!'.format(x.dim()))\n",
    "\n",
    "    x = x.view(-1, 1, h, w)\n",
    "    return x, b, c, h, w\n",
    "\n",
    "def reshape_output(\n",
    "        x: torch.Tensor, b: _I, c: _I) -> torch.Tensor:\n",
    "\n",
    "    rh = x.size(-2)\n",
    "    rw = x.size(-1)\n",
    "    # Back to the original dimension\n",
    "    if b is not None:\n",
    "        x = x.view(b, c, rh, rw)        # 4-dim\n",
    "    else:\n",
    "        if c is not None:\n",
    "            x = x.view(c, rh, rw)       # 3-dim\n",
    "        else:\n",
    "            x = x.view(rh, rw)          # 2-dim\n",
    "\n",
    "    return x\n",
    "\n",
    "def cast_input(x: torch.Tensor) -> typing.Tuple[torch.Tensor, _D]:\n",
    "    if x.dtype != torch.float32 or x.dtype != torch.float64:\n",
    "        dtype = x.dtype\n",
    "        x = x.float()\n",
    "    else:\n",
    "        dtype = None\n",
    "\n",
    "    return x, dtype\n",
    "\n",
    "def cast_output(x: torch.Tensor, dtype: _D) -> torch.Tensor:\n",
    "    if dtype is not None:\n",
    "        if not dtype.is_floating_point:\n",
    "            x = x.round()\n",
    "        # To prevent over/underflow when converting types\n",
    "        if dtype is torch.uint8:\n",
    "            x = x.clamp(0, 255)\n",
    "\n",
    "        x = x.to(dtype=dtype)\n",
    "\n",
    "    return x\n",
    "\n",
    "def resize_1d(\n",
    "        x: torch.Tensor,\n",
    "        dim: int,\n",
    "        size: typing.Optional[int],\n",
    "        scale: typing.Optional[float],\n",
    "        kernel: str='cubic',\n",
    "        sigma: float=2.0,\n",
    "        padding_type: str='reflect',\n",
    "        antialiasing: bool=True) -> torch.Tensor:\n",
    "\n",
    "    # Identity case\n",
    "    if scale == 1:\n",
    "        return x\n",
    "\n",
    "    # Default bicubic kernel with antialiasing (only when downsampling)\n",
    "    if kernel == 'cubic':\n",
    "        kernel_size = 4\n",
    "    else:\n",
    "        kernel_size = math.floor(6 * sigma)\n",
    "\n",
    "    if antialiasing and (scale < 1):\n",
    "        antialiasing_factor = scale\n",
    "        kernel_size = math.ceil(kernel_size / antialiasing_factor)\n",
    "    else:\n",
    "        antialiasing_factor = 1\n",
    "\n",
    "    # We allow margin to both sizes\n",
    "    kernel_size += 2\n",
    "\n",
    "    # Weights only depend on the shape of input and output,\n",
    "    # so we do not calculate gradients here.\n",
    "    with torch.no_grad():\n",
    "        pos = torch.linspace(\n",
    "            0, size - 1, steps=size, dtype=x.dtype, device=x.device,\n",
    "        )\n",
    "        pos = (pos + 0.5) / scale - 0.5\n",
    "        base = pos.floor() - (kernel_size // 2) + 1\n",
    "        dist = pos - base\n",
    "        weight = get_weight(\n",
    "            dist,\n",
    "            kernel_size,\n",
    "            kernel=kernel,\n",
    "            sigma=sigma,\n",
    "            antialiasing_factor=antialiasing_factor,\n",
    "        )\n",
    "        pad_pre, pad_post, base = get_padding(base, kernel_size, x.size(dim))\n",
    "\n",
    "    # To backpropagate through x\n",
    "    x_pad = padding(x, dim, pad_pre, pad_post, padding_type=padding_type)\n",
    "    unfold = reshape_tensor(x_pad, dim, kernel_size)\n",
    "    # Subsampling first\n",
    "    if dim == 2 or dim == -2:\n",
    "        sample = unfold[..., base, :]\n",
    "        weight = weight.view(1, kernel_size, sample.size(2), 1)\n",
    "    else:\n",
    "        sample = unfold[..., base]\n",
    "        weight = weight.view(1, kernel_size, 1, sample.size(3))\n",
    "\n",
    "    # Apply the kernel\n",
    "    x = sample * weight\n",
    "    x = x.sum(dim=1, keepdim=True)\n",
    "    return x\n",
    "\n",
    "def downsampling_2d(\n",
    "        x: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        scale: int,\n",
    "        padding_type: str='reflect') -> torch.Tensor:\n",
    "\n",
    "    c = x.size(1)\n",
    "    k_h = k.size(-2)\n",
    "    k_w = k.size(-1)\n",
    "\n",
    "    k = k.to(dtype=x.dtype, device=x.device)\n",
    "    k = k.view(1, 1, k_h, k_w)\n",
    "    k = k.repeat(c, c, 1, 1)\n",
    "    e = torch.eye(c, dtype=k.dtype, device=k.device, requires_grad=False)\n",
    "    e = e.view(c, c, 1, 1)\n",
    "    k = k * e\n",
    "\n",
    "    pad_h = (k_h - scale) // 2\n",
    "    pad_w = (k_w - scale) // 2\n",
    "    x = padding(x, -2, pad_h, pad_h, padding_type=padding_type)\n",
    "    x = padding(x, -1, pad_w, pad_w, padding_type=padding_type)\n",
    "    y = F.conv2d(x, k, padding=0, stride=scale)\n",
    "    return y\n",
    "\n",
    "def imresize(\n",
    "        x: torch.Tensor,\n",
    "        scale: typing.Optional[float]=None,\n",
    "        sizes: typing.Optional[typing.Tuple[int, int]]=None,\n",
    "        kernel: typing.Union[str, torch.Tensor]='cubic',\n",
    "        sigma: float=2,\n",
    "        rotation_degree: float=0,\n",
    "        padding_type: str='reflect',\n",
    "        antialiasing: bool=True) -> torch.Tensor:\n",
    "\n",
    "\n",
    "    if scale is None and sizes is None:\n",
    "        raise ValueError('One of scale or sizes must be specified!')\n",
    "    if scale is not None and sizes is not None:\n",
    "        raise ValueError('Please specify scale or sizes to avoid conflict!')\n",
    "\n",
    "    x, b, c, h, w = reshape_input(x)\n",
    "\n",
    "    if sizes is None:\n",
    "\n",
    "        # Determine output size\n",
    "        sizes = (math.ceil(h * scale), math.ceil(w * scale))\n",
    "        scales = (scale, scale)\n",
    "\n",
    "    if scale is None:\n",
    "        scales = (sizes[0] / h, sizes[1] / w)\n",
    "\n",
    "    x, dtype = cast_input(x)\n",
    "\n",
    "    if isinstance(kernel, str):\n",
    "        # Shared keyword arguments across dimensions\n",
    "        kwargs = {\n",
    "            'kernel': kernel,\n",
    "            'sigma': sigma,\n",
    "            'padding_type': padding_type,\n",
    "            'antialiasing': antialiasing,\n",
    "        }\n",
    "        # Core resizing module\n",
    "        x = resize_1d(x, -2, size=sizes[0], scale=scales[0], **kwargs)\n",
    "        x = resize_1d(x, -1, size=sizes[1], scale=scales[1], **kwargs)\n",
    "    elif isinstance(kernel, torch.Tensor):\n",
    "        x = downsampling_2d(x, kernel, scale=int(1 / scale))\n",
    "\n",
    "    x = reshape_output(x, b, c)\n",
    "    x = cast_output(x, dtype)\n",
    "    return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Just for debugging\n",
    "    torch.set_printoptions(precision=4, sci_mode=False, edgeitems=16, linewidth=200)\n",
    "    a = torch.arange(64).float().view(1, 1, 8, 8)\n",
    "    z = imresize(a, 0.5)\n",
    "    print(z)\n",
    "    #a = torch.arange(16).float().view(1, 1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20254,
     "status": "ok",
     "timestamp": 1620638210634,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "KwbJIrLyEMyL",
    "outputId": "7e4f4a59-cdb7-4562-fd99-758e2c956c66"
   },
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)  # download and load pretrained model\n",
    "resnet50.eval()\n",
    "\n",
    "# vgg16 = models.vgg16(pretrained=True)  # download and load pretrained model\n",
    "# vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20249,
     "status": "ok",
     "timestamp": 1620638210634,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "S1D3f3mvEMyN"
   },
   "outputs": [],
   "source": [
    "## mean and std will remain same irresptive of the model you use\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess11 = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "labels_link = \"https://savan77.github.io/blog/labels.json\"\n",
    "labels_json = requests.get(labels_link).json()\n",
    "labels = {int(idx): label for idx, label in labels_json.items()}\n",
    "\n",
    "## Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "# manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20238,
     "status": "ok",
     "timestamp": 1620638210636,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "zN3bPytnEMyO",
    "outputId": "93020c72-0e6c-40be-9d87-755a9af97a22"
   },
   "outputs": [],
   "source": [
    "padsize = 1\n",
    "m = 224  #Pixel number of SLM\n",
    "factor = 2  #For Nyquist sampling\n",
    "factor2 = 1\n",
    "M = m * factor * factor2  #Image size\n",
    "f = 100000  #Focal length of relay lens in um\n",
    "length = [0.61, 0.53, 0.47]  #Wavelength in um [Red Green Blue]. Note that center wavelegth is green color.\n",
    "length = np.array(length)\n",
    "fov = 250000  #Field-of-view in um\n",
    "eff_pixelsize = fov/M  #Effective pixel size\n",
    "slm_pixelsize = 30  #Pixel size of SLM in um\n",
    "slm_pixelnum = m  #추가\n",
    "\n",
    "PhySizeSLM = slm_pixelnum * slm_pixelsize\n",
    "MaxFreqSLM = PhySizeSLM / length / f / 2\n",
    "DelFreqSLM = slm_pixelsize / np.min(length) / f  #기준은 가장 High frequency를 전달할 수 있는 Blue channel\n",
    "\n",
    "kx = (DelFreqSLM * torch.range(-M / 2, M / 2 - 1)).to(torch.double)\n",
    "ky = (DelFreqSLM * torch.range(-M / 2, M / 2 - 1)).to(torch.double)\n",
    "kxm, kym = torch.meshgrid(kx, ky)\n",
    "\n",
    "Ratio = MaxFreqSLM / MaxFreqSLM[2]  #Radius ratio of each color channel\n",
    "\n",
    "cent = M/2\n",
    "initial = torch.rand(224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20231,
     "status": "ok",
     "timestamp": 1620638210636,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "aYPsPOpNEMyP"
   },
   "outputs": [],
   "source": [
    "## Random perturbation\n",
    "\n",
    "class FourierDomain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierDomain, self).__init__()\n",
    "        self.MM = torch.nn.Parameter(initial)\n",
    "        self.MM.requires_grad = True\n",
    "\n",
    "    def forward(self, inputIntensityFT, attack):\n",
    "        slm = self.MM\n",
    "        \n",
    "        slm = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=factor2)))\n",
    "        CTF_B = (torch.sqrt(torch.pow(kxm, 2) + torch.pow(kym, 2)) < MaxFreqSLM[2] * factor2).to(torch.double)\n",
    "        CTF_B = CTF_B.to(device)\n",
    "        slm = slm * CTF_B[int(cent-M/4):int(cent+M/4),int(cent-M/4):int(cent+M/4)]  #Physicially displayed SLM pattern\n",
    "        \n",
    "        slm_R = Ratio[0] * torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=Ratio[0])))\n",
    "        slm_G = Ratio[1] * torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(slm, 0), 0), scale_factor=Ratio[1])))\n",
    "        slm_B = slm\n",
    "        \n",
    "        slm_r = torch.zeros(M, M).to(device)\n",
    "        slm_g = torch.zeros(M, M).to(device)\n",
    "        slm_b = torch.zeros(M, M).to(device)\n",
    "        \n",
    "        x, y = tuple(list(slm_R.size()))\n",
    "        slm_r[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_R\n",
    "\n",
    "        x, y = tuple(list(slm_G.size()))\n",
    "        slm_g[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_G\n",
    "\n",
    "        x, y = tuple(list(slm_B.size()))\n",
    "        slm_b[int(M / 2 + 1 + floor(-y / 2)):int(M / 2 + 1 + floor(y / 2)),\n",
    "        int(M / 2 + 1 + floor(-x / 2)):int(M / 2 + 1 + floor(x / 2))] = slm_B\n",
    "        \n",
    "        CTF_R = (slm_r != 0).to(torch.double)\n",
    "        CTF_G = (slm_g != 0).to(torch.double)\n",
    "        CTF_B = (slm_b != 0).to(torch.double)\n",
    "        \n",
    "        atk_CTF_R_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_r), 2), torch.unsqueeze(torch.sin(slm_r), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_R_mask = torch.view_as_complex(atk_CTF_R_mask_stack)\n",
    "        atk_CTF_R = CTF_R * atk_CTF_R_mask\n",
    "\n",
    "        atk_CTF_G_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_g), 2), torch.unsqueeze(torch.sin(slm_g), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_G_mask = torch.view_as_complex(atk_CTF_G_mask_stack)\n",
    "        atk_CTF_G = CTF_G * atk_CTF_G_mask\n",
    "\n",
    "        atk_CTF_B_mask_stack = torch.cat([torch.unsqueeze(torch.cos(slm_b), 2), torch.unsqueeze(torch.sin(slm_b), 2)],\n",
    "                                         dim=2)\n",
    "        atk_CTF_B_mask = torch.view_as_complex(atk_CTF_B_mask_stack)\n",
    "        atk_CTF_B = CTF_B * atk_CTF_B_mask\n",
    "        \n",
    "        if attack == True:\n",
    "            ft_atk_CTF_R = fftn(fftshift(atk_CTF_R))\n",
    "            MTF_R = ifftshift(ifftn((ft_atk_CTF_R * torch.conj(ft_atk_CTF_R))))\n",
    "\n",
    "            ft_atk_CTF_G = fftn(fftshift(atk_CTF_G))\n",
    "            MTF_G = ifftshift(ifftn((ft_atk_CTF_G * torch.conj(ft_atk_CTF_G))))\n",
    "\n",
    "            ft_atk_CTF_B = fftn(fftshift(atk_CTF_B))\n",
    "            MTF_B = ifftshift(ifftn((ft_atk_CTF_B * torch.conj(ft_atk_CTF_B))))\n",
    "\n",
    "        elif attack == False:\n",
    "            ft_CTF_R = fftn(fftshift(CTF_R))\n",
    "            MTF_R = ifftshift(ifftn((ft_CTF_R * torch.conj(ft_CTF_R))))\n",
    "\n",
    "            ft_CTF_G = fftn(fftshift(CTF_G))\n",
    "            MTF_G = ifftshift(ifftn((ft_CTF_G * torch.conj(ft_CTF_G))))\n",
    "\n",
    "            ft_CTF_B = fftn(fftshift(CTF_B))\n",
    "            MTF_B = ifftshift(ifftn((ft_CTF_B * torch.conj(ft_CTF_B))))\n",
    "\n",
    "        MTF_R = MTF_R / torch.max(torch.max(torch.abs(MTF_R)))\n",
    "        MTF_G = MTF_G / torch.max(torch.max(torch.abs(MTF_G)))\n",
    "        MTF_B = MTF_B / torch.max(torch.max(torch.abs(MTF_B)))\n",
    "\n",
    "        #inputIntensityFT = torch.as_tensor(inputIntensityFT, dtype=torch.cdouble)\n",
    "        inputIntensityFT_R = inputIntensityFT[:, :, 0]\n",
    "        inputIntensityFT_G = inputIntensityFT[:, :, 1]\n",
    "        inputIntensityFT_B = inputIntensityFT[:, :, 2]\n",
    "        \n",
    "        outputFT_R = MTF_R * inputIntensityFT_R\n",
    "        outputFT_G = MTF_G * inputIntensityFT_G\n",
    "        outputFT_B = MTF_B * inputIntensityFT_B\n",
    "        \n",
    "        ifft_R = torch.abs(ifftn(ifftshift(outputFT_R))).to(torch.float32) / 255\n",
    "        ifft_G = torch.abs(ifftn(ifftshift(outputFT_G))).to(torch.float32) / 255\n",
    "        ifft_B = torch.abs(ifftn(ifftshift(outputFT_B))).to(torch.float32) / 255\n",
    "                \n",
    "        output_R = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_R, 0), 0), size=(224, 224))))\n",
    "        output_G = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_G, 0), 0), size=(224, 224))))\n",
    "        output_B = torch.squeeze(torch.squeeze(F.interpolate(torch.unsqueeze(torch.unsqueeze(ifft_B, 0), 0), size=(224, 224))))\n",
    "    \n",
    "        image_tensor2 = torch.stack([output_R, output_G, output_B])\n",
    "        \n",
    "    \n",
    "        return image_tensor2, slm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20633,
     "status": "ok",
     "timestamp": 1620638211042,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "wcztXDdiEMyS"
   },
   "outputs": [],
   "source": [
    "## find inputIntensityFT\n",
    "\n",
    "img = Image.open(filepath)\n",
    "tf = transforms.ToTensor()\n",
    "IMG = tf(img)\n",
    "IMG2 = imresize(IMG, sizes=(M, M))\n",
    "IMG2 = IMG2.unsqueeze(0)\n",
    "inputIntensityFT_R = fftshift(fftn(IMG2[:,0,:,:])).squeeze(0)\n",
    "inputIntensityFT_G = fftshift(fftn(IMG2[:,1,:,:])).squeeze(0)\n",
    "inputIntensityFT_B = fftshift(fftn(IMG2[:,2,:,:])).squeeze(0)\n",
    "inputIntensityFT = torch.stack([inputIntensityFT_R, inputIntensityFT_G, inputIntensityFT_B])\n",
    "inputIntensityFT = (inputIntensityFT * 255).permute(1, 2, 0)\n",
    "inputIntensityFT = inputIntensityFT.detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 21698,
     "status": "ok",
     "timestamp": 1620638212113,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "PHSXsEu_EMyS",
    "outputId": "45d92f08-408d-4860-a21f-78cd14457a05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## original image(ground truth)\n",
    "\n",
    "resnet50.to(device)\n",
    "image_tensor = imresize(IMG, sizes=(224, 224))\n",
    "image_tensor = image_tensor.detach().to(device)\n",
    "image_tensor = preprocess11(image_tensor)  # preprocess an i\n",
    "image_tensor = image_tensor.unsqueeze(0)  # add batch dimension.  C X H X W ==> B X C X H X W\n",
    "output = resnet50.forward(image_tensor)\n",
    "label_idx = torch.max(output.data, 1)[1][0]  # get an index(class number) of a largest element\n",
    "label_idx = label_idx.item()\n",
    "x_pred = labels[label_idx]\n",
    "y_true = label_idx  ##change this if you change input image\n",
    "target = Variable(torch.LongTensor([y_true]).to(device), requires_grad=False)\n",
    "\n",
    "output_probs = F.softmax(output, dim=1)\n",
    "x_pred_prob = np.round((torch.max(output_probs.data, 1)[0][0]).cpu() * 100, 4)\n",
    "x_pred_prob = x_pred_prob.item()\n",
    "\n",
    "print(x_pred)\n",
    "print(x_pred_prob)\n",
    "\n",
    "x = image_tensor.squeeze(0)\n",
    "x = x.mul((torch.FloatTensor(std)).to(device).view(3, 1, 1)).add(\n",
    "    (torch.FloatTensor(mean)).to(device).view(3, 1, 1)).cpu().numpy()  # reverse of normalization op- \"unnormalize\"\n",
    "x = np.transpose(x, (1, 2, 0))  # C X H X W  ==>   H X W X C\n",
    "x = np.clip(x, 0, 1)\n",
    "\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234352,
     "status": "ok",
     "timestamp": 1620638424774,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "H-CgplMREMyU",
    "outputId": "e97707d6-8081-41fd-b105-f7a5aa641987",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Perturbation update\n",
    "\n",
    "x_adv_pred = x_pred\n",
    "param = 1e-2\n",
    "n_epoch = 150\n",
    "a = 0\n",
    "i = 0\n",
    "\n",
    "while x_adv_pred == x_pred:\n",
    "\n",
    "    initial = torch.rand(224, 224).to(device)\n",
    "    model = FourierDomain()\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5 * (1e-3), weight_decay=5e-6)\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_tensor2, slm = model(inputIntensityFT, attack=True)\n",
    "        image_tensor2 = preprocess11(image_tensor2)  # preprocess an i\n",
    "        image_tensor2 = image_tensor2.unsqueeze(0)\n",
    "        output2 = resnet50.forward(image_tensor2)\n",
    "\n",
    "        # perform a backward pass in order to get gradients 670 2000\n",
    "        loss2 = param * torch.norm(image_tensor2 - image_tensor) - criterion(output2, target)\n",
    "        \n",
    "        x_adv_pred = labels[torch.max(output2.data, 1)[1][0].item()]  # classify adversarial example\n",
    "        if x_adv_pred != x_pred:\n",
    "            break\n",
    "\n",
    "        loss2.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if x_adv_pred != x_pred:\n",
    "        break\n",
    "        \n",
    "    param /= 10\n",
    "    a += 1\n",
    "\n",
    "    if a == 5:\n",
    "        break\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "print('param: ', 1e-2 / (10**(i//150)), 'epoch: ', i%150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 234349,
     "status": "ok",
     "timestamp": 1620638424775,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "kwlYmK0WEMyU",
    "outputId": "312b41f5-c473-435d-bc00-b404d84a74bb"
   },
   "outputs": [],
   "source": [
    "## adversarial image\n",
    "output_adv_probs = F.softmax(output2, dim=1)\n",
    "x_adv_pred_prob = np.round((torch.max(output_adv_probs.data, 1)[0][0]).cpu() * 100, 4)\n",
    "x_adv_pred_prob = x_adv_pred_prob.item()\n",
    "\n",
    "print(x_adv_pred)\n",
    "print(x_adv_pred_prob)\n",
    "\n",
    "x2 = image_tensor2.squeeze(0)\n",
    "x2 = x2.mul((torch.FloatTensor(std)).to(device).view(3, 1, 1)).add(\n",
    "    (torch.FloatTensor(mean)).to(device).view(3, 1, 1)).cpu().detach().numpy()  # reverse of normalization op- \"unnormalize\"\n",
    "x2 = np.transpose(x2, (1, 2, 0))  # C X H X W  ==>   H X W X C\n",
    "x2 = np.clip(x2, 0, 1)\n",
    "\n",
    "plt.imsave(adv_filepath, x2)\n",
    "plt.imshow(x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234895,
     "status": "ok",
     "timestamp": 1620638425333,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "jJi56kh2ZjHa",
    "outputId": "3ef65f92-ae15-4be3-a56a-f2595082a58e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Image quality assessment\n",
    "\n",
    "dist = torch.as_tensor(x2).permute(2, 0, 1).unsqueeze(0)\n",
    "ref = torch.as_tensor(x).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "model1 = SSIM(channels=3) # 0~1 값, 1에 가까울수록 원본과 유사도 높음\n",
    "score1 = model1(dist, ref, as_loss=False)\n",
    "\n",
    "model2 = VIF(channels=3) # 0~1 값, 1에 가까울수록 원본과 유사도 높음, 원본보다 이미지 퀄리티 좋을경우 1보다 클 수 있음\n",
    "score2 = model2(dist, ref, as_loss=False)\n",
    "\n",
    "print('SSIM of ResNet50: ', score1)\n",
    "print('VIF of ResNet50: ', score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234891,
     "status": "ok",
     "timestamp": 1620638425334,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "YsZzC5YXaryQ",
    "outputId": "f6bb5b23-d846-49b2-dbe8-d82450b4fc29"
   },
   "outputs": [],
   "source": [
    "## saving slm pattern\n",
    "M = slm.cpu().detach().numpy()\n",
    "df = pd.DataFrame(M)\n",
    "df.to_excel(adv_filepath2, header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 236287,
     "status": "ok",
     "timestamp": 1620638426735,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "4rysmZHY0_ju"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 236284,
     "status": "ok",
     "timestamp": 1620638426735,
     "user": {
      "displayName": "‍김정수(학부학생/공과대학 기계공학)",
      "photoUrl": "",
      "userId": "11518952761035852619"
     },
     "user_tz": -540
    },
    "id": "10cgccqB0_l9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_experiment_code_cuda_edit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
